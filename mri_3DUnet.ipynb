{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "mri_3DUnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yerzhanimanmalik/skoltech-courses/blob/master/mri_3DUnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lny-_CwLFxVM"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1DedzlRT-yCVz4cyZmy05TblRspbiVIBU\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6YeoOEpJpy4"
      },
      "source": [
        "# **MRI segmentation with 3D U-net**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LqQkmVuLi6o"
      },
      "source": [
        "#### 1. Introduction\n",
        "\n",
        "In this section we will segment gray matter and subcortical nuclei from preprocessed MRI image.\n",
        "\n",
        "*Proceeding with this Notebook you confirm your personal acess [to the data](https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release). \n",
        " And your agreement on data [terms and conditions](https://www.humanconnectome.org/study/hcp-young-adult/data-use-terms).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSzOne1OItfJ"
      },
      "source": [
        "import scipy as sp\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZVui824s_Mr",
        "outputId": "16865a75-6aa2-484f-d558-140ef57a9768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install --quiet --upgrade comet_ml\n",
        "from comet_ml import Experiment\n",
        "    \n",
        "# Add the following code anywhere in your machine learning file\n",
        "experiment = Experiment(api_key=\"kYVqzmHEUN7WQLo86k2bZs1Z7\",\n",
        "                        project_name=\"mri-segmentation\", workspace=\"kondratevakate\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 225kB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 13.6MB/s \n",
            "\u001b[?25h  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/kondratevakate/mri-segmentation/e87f0b3e9ce44552b89b133106c65c41\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii8ogiHEONTi"
      },
      "source": [
        "The code is adopted from `Torchio` library tutorials, and used in MRI epilepsy detection experiment. See https://github.com/kondratevakate/mri-epilepsy-segmentation\n",
        "\n",
        "!! And tutorial: https://colab.research.google.com/drive/112NTL8uJXzcMw4PQbUvMQN-WHlVwQS3i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evtOoVX_PDvX"
      },
      "source": [
        "#### 2. Mounting Google drive\n",
        "\n",
        "Mounting Google Drive to Collab Notebook. You should go with the link and enter your personal authorization code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT1UYpwaItfS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zLyMt9CPpOI"
      },
      "source": [
        "data_dir = '/content/drive/My Drive/data/seminars/anat/fs_segmentation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abYD1633PqJF"
      },
      "source": [
        "len(os.listdir(data_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV_jmg7lQC0G"
      },
      "source": [
        "os.listdir(data_dir)[-30:-20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6xjOyP3Qs8X"
      },
      "source": [
        "#### 3. Defining the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUl_R5bgRyav"
      },
      "source": [
        "Defining the working dataset, there:\n",
        "\n",
        " 1. `norm` - normalised `T1` image processed with Freesurfer 6.0,\n",
        "\n",
        " 2. `aparc+aseg` segmentation mask for gray matter and subcortical nuclei from Freesufer 6.0 `recon all` pipeline.\n",
        "\n",
        " And U-net model will treat `norm` image as input and `aparc+aseg` as target model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU3ZwxcrRx6-"
      },
      "source": [
        "import pandas as pd\n",
        "labels_dir  = '/content/drive/My Drive/data/seminars/anat/'\n",
        "labels = pd.read_csv(labels_dir + 'unrestricted_hcp_freesurfer.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eBoVB9ZMu4b"
      },
      "source": [
        "Defining new `pd.Dataframe()` with `Subject`, `norm` and `target` files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfWcsdESMolg"
      },
      "source": [
        "data_list = pd.DataFrame(columns = ['Subject','norm','aseg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMI78_wsL1Ff"
      },
      "source": [
        "data_list['Subject'] = labels['Subject']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y78CEixkXxf0"
      },
      "source": [
        "Iterating through files and `Subjects` in ID list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x06eCtN9Xcet"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(os.listdir(data_dir)):\n",
        "  for j in range(0, len(data_list['Subject'])):\n",
        "\n",
        "    if str(data_list['Subject'].iloc[j]) in i:\n",
        "      if 'norm' in i: # copying path to the column norm\n",
        "        data_list['norm'].iloc[j] = data_dir +'/'+ i\n",
        "      elif 'aseg' in i: # copying path to second column\n",
        "        data_list['aseg'].iloc[j] = data_dir +'/'+ i\n",
        "      \n",
        "data_list.dropna(inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eey6753TOthV"
      },
      "source": [
        "data_list.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKEpEH31Q9YD"
      },
      "source": [
        "Let's have a closer look on the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZKNvIUuRaxD"
      },
      "source": [
        "data_list['norm'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORhOxckhQ8T4"
      },
      "source": [
        "!pip install --quiet --upgrade nilearn\n",
        "import nilearn\n",
        "from nilearn import plotting\n",
        "\n",
        "# visualising normalised image\n",
        "img = nilearn.image.load_img(data_list['norm'].iloc[0])\n",
        "plotting.plot_anat(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja5weGNzRnkx"
      },
      "source": [
        "# visualising segmentation\n",
        "img = nilearn.image.load_img(data_list['aseg'].iloc[0])\n",
        "plotting.plot_anat(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU0kWyDMR6w4"
      },
      "source": [
        "np.unique(img.get_data())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DApgjc8gSGD1"
      },
      "source": [
        "Any suggestions on these numbers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUZAi8kCQQA_"
      },
      "source": [
        "#### 4. Writing dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrYEX6rRRosY"
      },
      "source": [
        "We will use `TorchIO` library: https://torchio.readthedocs.io/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1vauzBnRMhA"
      },
      "source": [
        "!pip install --quiet --upgrade torchio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY1xztsiQPPO"
      },
      "source": [
        "import torchio \n",
        "import enum\n",
        "\"\"\"\n",
        "    Code adapted from: https://github.com/fepegar/torchio#credits\n",
        "\n",
        "        Credit: Pérez-García et al., 2020, TorchIO: \n",
        "        a Python library for efficient loading, preprocessing, \n",
        "        augmentation and patch-based sampling of medical images in deep learning.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "CHANNELS_DIMENSION = 1\n",
        "SPATIAL_DIMENSIONS = 2, 3, 4\n",
        "\n",
        "MRI = 'MRI'\n",
        "LABEL = 'LABEL'\n",
        "\n",
        "LIST_ASEG =  [ 8,   10,   11,   12,   13,    16,   17,   18,  26,  47, 49,   50, \n",
        "  51,   52,   53,   54,   58,  85,  251,  252,  253,  254,  255] # gray matter segmentation labels\n",
        "\n",
        "class Action(enum.Enum):\n",
        "    TRAIN = 'Training'\n",
        "    VALIDATE = 'Validation'\n",
        "\n",
        "def get_torchio_dataset(inputs, targets, transform):\n",
        "    \"\"\"\n",
        "    The function creates dataset from the list of files from cunstumised dataloader.\n",
        "    \"\"\"\n",
        "    subjects = []\n",
        "    for (image_path, label_path) in zip(inputs, targets):\n",
        "        subject_dict = {\n",
        "            MRI : torchio.Image(image_path, torchio.INTENSITY),\n",
        "            LABEL: torchio.Image(label_path, torchio.LABEL),\n",
        "        }\n",
        "        subject = torchio.Subject(subject_dict)\n",
        "        subjects.append(subject)\n",
        "    \n",
        "    if transform:\n",
        "        dataset = torchio.SubjectsDataset(subjects, transform = transform)\n",
        "    elif not transform:\n",
        "        dataset = torchio.SubjectsDataset(subjects)\n",
        "    \n",
        "    return  dataset , subjects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCUWSvOHkASt"
      },
      "source": [
        "data, subjects = get_torchio_dataset(data_list['norm'], data_list['aseg'], False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_soFhcYkijr"
      },
      "source": [
        "data[0]['MRI']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_JR8HbiPCGl"
      },
      "source": [
        "#### 3. Writing visualization tools for torch tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f26Pc35TPAaO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import nibabel\n",
        "\n",
        "def plot_central_cuts(img, title=\"\"):\n",
        "    \"\"\"\n",
        "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
        "    \"\"\"\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.numpy()\n",
        "        if (len(img.shape) > 3):\n",
        "            img = img[0,:,:,:]\n",
        "                \n",
        "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
        "        img = img.get_fdata()\n",
        "   \n",
        "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 4, 4))\n",
        "    axes[0].imshow(img[ img.shape[0] // 2, :, :])\n",
        "    axes[1].imshow(img[ :, img.shape[1] // 2, :])\n",
        "    axes[2].imshow(img[ :, :, img.shape[2] // 2])\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "def plot_predicted(img, seg, delta = 0, title=\"\"):\n",
        "    \"\"\"\n",
        "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
        "    \"\"\"\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.cpu().numpy()\n",
        "        if (len(img.shape) == 5):\n",
        "            img = img[0,0,:,:,:]\n",
        "        elif (len(img.shape) == 4):\n",
        "            img = img[0,:,:,:]\n",
        "                \n",
        "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
        "        img = img.get_fdata()\n",
        "        \n",
        "    if isinstance(seg, torch.Tensor):\n",
        "        seg= seg[0].cpu().numpy().astype(np.uint8)\n",
        "   \n",
        "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 4, 4))\n",
        "    axes[0].imshow(img[ img.shape[0] // 2 + delta, :, :])\n",
        "    axes[1].imshow(seg[ seg.shape[0] // 2 + delta, :, :])\n",
        "    intersect = img[ img.shape[0] // 2 + delta, :, :] + seg[ seg.shape[0] // 2 + delta, :, :]*100\n",
        "    axes[2].imshow(intersect, cmap='gray')\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9YsFfH8QNXm"
      },
      "source": [
        "The class `dataset` inherits from `torch.utils.data.Dataset.` It receives as input a list of torchio.Subject instances and an optional `torchio.transforms.Transform.`\n",
        "\n",
        "The inputs to the subject class are instances of torchio.Image, such as torchio.ScalarImage or torchio.LabelMap. The image class will be used by the transforms to decide whether or not to perform the operation. For example, spatial transforms must apply to both, but intensity transforms must apply to scalar images only.\n",
        "\n",
        "https://torchio.readthedocs.io/data/dataset.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCQr-LiJrRML"
      },
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchio import AFFINE, DATA, PATH, TYPE, STEM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Cr6UL5ItfU"
      },
      "source": [
        "print(\"Dataset size: {}\".format(len(data)))\n",
        "img = data[0][MRI]\n",
        "seg = data[0][LABEL]\n",
        "print(\"Image shape: {}\".format(img.shape))\n",
        "print(\"Segmentation shape: {}\".format(seg.shape))\n",
        "plot_central_cuts(img[DATA])\n",
        "plot_central_cuts(seg[DATA])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGRajfc3Itfc"
      },
      "source": [
        "## 2. Whole brain segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irNwaqb6_q-c"
      },
      "source": [
        "Let's define the experiment for whole brain segmentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um7RyaNq_rY2"
      },
      "source": [
        "experiment.set_name(\"Whole image based, 1 subject in batch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xODpTdPbvK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models.vgg import vgg11_bn\n",
        "from torch.autograd import Function, Variable\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from optparse import OptionParser\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch import optim\n",
        "import time\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4fivIfjItfe"
      },
      "source": [
        "!pip install --quiet --upgrade unet \n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from unet import UNet\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, ShuffleSplit\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import warnings\n",
        "import multiprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x524TiSBt7KK"
      },
      "source": [
        "num_subjects = len(data)\n",
        "training_split_ratio = 1/2\n",
        "num_training_subjects = int(training_split_ratio * num_subjects)\n",
        "\n",
        "# training_subjects = subjects[:num_training_subjects]\n",
        "# validation_subjects = subjects[num_training_subjects:]\n",
        "\n",
        "training_subjects = subjects[:20]\n",
        "validation_subjects = subjects[20:40] # experimenting just on 20 first subjects\n",
        "\n",
        "training_set = torchio.SubjectsDataset(\n",
        "    training_subjects, transform=None)\n",
        "\n",
        "validation_set = torchio.SubjectsDataset(\n",
        "    validation_subjects, transform=None)\n",
        "\n",
        "print('Training set:', len(training_set), 'subjects')\n",
        "print('Validation set:', len(validation_set), 'subjects')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSJoCf0PzA4V"
      },
      "source": [
        "The data is really heavy, so lets try to start with 1 subject/ batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3iQ-5DxytM0"
      },
      "source": [
        "training_batch_size = 1\n",
        "validation_batch_size = 1\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    training_set,\n",
        "    batch_size=training_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=multiprocessing.cpu_count(),\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_set,\n",
        "    batch_size=validation_batch_size,\n",
        "    num_workers=multiprocessing.cpu_count(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olI1JmOGvotO"
      },
      "source": [
        "#### Defining the model and optimizer for training\n",
        "\n",
        "At first check if we have GPU onborad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I-NoHhMwHMl"
      },
      "source": [
        " torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46QNsxiDxpcy"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTvcHLCFyEEu"
      },
      "source": [
        "def get_iou_score(prediction, ground_truth):\n",
        "    intersection, union = 0, 0\n",
        "    intersection += np.logical_and(prediction > 0, ground_truth > 0).astype(np.float32).sum() \n",
        "    union += np.logical_or(prediction > 0, ground_truth > 0).astype(np.float32).sum()\n",
        "    iou_score = float(intersection) / union\n",
        "    return iou_score\n",
        "\n",
        "def calculate_metrics(surface, prediction):\n",
        "    dsc = compute_dice_coefficient(surface, prediction)\n",
        "    asd_mean, asd_std = compute_average_surface_distance(\n",
        "        compute_surface_distances(\n",
        "            surface, prediction, spacing_mm=(1,1,1))\n",
        "    )\n",
        "    \n",
        "    iou = get_iou_score(prediction, surface)\n",
        "    \n",
        "    return dsc, asd_mean, asd_std, iou        \n",
        "        \n",
        "## see more in https://github.com/deepmind/surface-distance\n",
        "def validate_dsc_asd(model, loader):\n",
        "    \n",
        "    dsc, asd_mean, asd_std, iou = [], [], [], []\n",
        "    model.eval()\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(loader)):\n",
        "            inputs, targets = prepare_batch(batch, device)\n",
        "            with torch.no_grad():\n",
        "                logits = forward(model, inputs)\n",
        "            labels = logits.argmax(dim=CHANNELS_DIMENSION)\n",
        "            prediction = labels[0].cpu().numpy().astype(np.uint8)\n",
        "            dsc_temp, asd_mean_temp, asd_std_temp, iou_temp = calculate_metrics(\n",
        "                targets.cpu().numpy().astype(np.uint8)[0][0], \n",
        "                prediction\n",
        "            )\n",
        "            dsc.append(dsc_temp)\n",
        "            asd_mean.append(asd_mean_temp)\n",
        "            asd_std.append(asd_std_temp)\n",
        "            iou.append(iou_temp)\n",
        "    \n",
        "    return dsc, asd_mean, asd_std, iou\n",
        "\n",
        "def prepare_batch(batch, device):\n",
        "    \"\"\"\n",
        "    The function loaging *nii.gz files, sending to the devise.\n",
        "    For the LABEL in binarises the data.\n",
        "    \"\"\"\n",
        "    inputs = batch[MRI][DATA].to(device)\n",
        "    targets = batch[LABEL][DATA]\n",
        "    targets[0][0][(np.isin(targets[0][0], LIST_ASEG))] = 1\n",
        "    targets[targets >= 1000] = 1\n",
        "    targets[targets != 1] = 0\n",
        "    targets = targets.to(device)    \n",
        "    return inputs, targets\n",
        "\n",
        "def get_dice_score(output, target, SPATIAL_DIMENSIONS = (2, 3, 4), epsilon=1e-9):\n",
        "    p0 = output\n",
        "    g0 = target\n",
        "    p1 = 1 - p0\n",
        "    g1 = 1 - g0\n",
        "    tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
        "    num = 2 * tp\n",
        "    denom = 2 * tp + fp + fn + epsilon\n",
        "    dice_score = num / denom\n",
        "    return dice_score\n",
        "\n",
        "def get_dice_loss(output, target):\n",
        "    return 1 - get_dice_score(output, target)\n",
        "\n",
        "def forward(model, inputs):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "        logits = model(inputs)\n",
        "    return logits\n",
        "\n",
        "def run_epoch(epoch_idx, action, loader, model, optimizer, scheduler=False, experiment= False):\n",
        "    is_training = action == Action.TRAIN\n",
        "    epoch_losses = []\n",
        "    model.train(is_training)\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(loader)):\n",
        "        inputs, targets = prepare_batch(batch, device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            logits = forward(model, inputs)\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets)\n",
        "            batch_loss = batch_losses.mean()\n",
        "            \n",
        "            if is_training:\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            # appending the loss\n",
        "            epoch_losses.append(batch_loss.item())\n",
        "           \n",
        "            if experiment:\n",
        "                if action == Action.TRAIN:\n",
        "                    experiment.log_metric(\"train_dice_loss\", batch_loss.item())\n",
        "                elif action == Action.VALIDATE:\n",
        "                    experiment.log_metric(\"validate_dice_loss\", batch_loss.item())\n",
        "                    \n",
        "            del inputs, targets, logits, probabilities, batch_losses\n",
        "    \n",
        "    epoch_losses = np.array(epoch_losses)\n",
        "#     print(f'{action.value} mean loss: {epoch_losses.mean():0.3f}')\n",
        "    \n",
        "    return epoch_losses \n",
        "\n",
        "def train(num_epochs, training_loader, validation_loader, model, optimizer, scheduler,\n",
        "          weights_stem, save_epoch= 1, experiment= False, verbose = True):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    epoch_train_loss, epoch_val_loss = [], []\n",
        "    \n",
        "    run_epoch(0, Action.VALIDATE, validation_loader, model, optimizer, scheduler, experiment)\n",
        "    \n",
        "    for epoch_idx in range(1, num_epochs + 1):\n",
        "        \n",
        "        epoch_train_losses = run_epoch(epoch_idx, Action.TRAIN, training_loader, \n",
        "                                       model, optimizer, scheduler, experiment)\n",
        "        epoch_val_losses = run_epoch(epoch_idx, Action.VALIDATE, validation_loader, \n",
        "                                     model, optimizer, scheduler, experiment)\n",
        "        \n",
        "        # 4. Print metrics\n",
        "        if verbose:\n",
        "            clear_output(True)\n",
        "            print(\"Epoch {} of {} took {:.3f}s\".format(epoch_idx, num_epochs, time.time() - start_time))\n",
        "            print(\"  training loss (in-iteration): \\t{:.6f}\".format(epoch_train_losses[-1]))\n",
        "            print(\"  validation loss: \\t\\t\\t{:.6f}\".format(epoch_val_losses[-1]))    \n",
        "        \n",
        "        epoch_train_loss.append(np.mean(epoch_train_losses))\n",
        "        epoch_val_loss.append(np.mean(epoch_val_losses))\n",
        "        \n",
        "        # 5. Plot metrics\n",
        "        if verbose:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(epoch_train_loss, label='train')\n",
        "            plt.plot(epoch_val_loss, label='val')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        \n",
        "        if scheduler:     \n",
        "            scheduler.step(np.mean(epoch_val_losses))\n",
        "        # comet ml\n",
        "        if experiment:\n",
        "            experiment.log_epoch_end(epoch_idx)\n",
        "        #saving the model\n",
        "        # if (epoch_idx% save_epoch == 0):\n",
        "        #     torch.save(model.state_dict(), f'weights/{weights_stem}_epoch_{epoch_idx}.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CTdHHAt0aYW"
      },
      "source": [
        "def get_model_and_optimizer(device, num_encoding_blocks=3, out_channels_first_layer=4, patience=3):\n",
        "  #Better to train with num_encoding_blocks=3, out_channels_first_layer=16 '''\n",
        "  #repoducibility\n",
        "  torch.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "  model = UNet(\n",
        "        in_channels=1,\n",
        "        out_classes=2,\n",
        "        dimensions=3,\n",
        "        num_encoding_blocks=num_encoding_blocks,\n",
        "        out_channels_first_layer=out_channels_first_layer,\n",
        "        normalization='batch',\n",
        "        upsampling_type='linear',\n",
        "        padding=True,\n",
        "        activation='PReLU',\n",
        "    ).to(device)\n",
        "    \n",
        "  optimizer = torch.optim.AdamW(model.parameters())\n",
        "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, threshold=0.01)\n",
        "    \n",
        "  return model, optimizer, scheduler\n",
        "\n",
        "model, optimizer, scheduler = get_model_and_optimizer(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFazOG6BItfj"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "num_epochs = 5\n",
        "weights_stem = 'whole_images'\n",
        "train(num_epochs, training_loader, validation_loader, model, optimizer, scheduler = False,  weights_stem = weights_stem )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FXpfIrx5x85"
      },
      "source": [
        "We expect to get `DICE > 0.9` on validation after 40 epochs on the whole sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSTAYei6Itfl"
      },
      "source": [
        "batch = next(iter(validation_loader))\n",
        "model.eval()\n",
        "inputs, targets = prepare_batch(batch, device)\n",
        "with torch.no_grad():\n",
        "    logits = forward(model, inputs)\n",
        "labels = logits.argmax(dim=CHANNELS_DIMENSION)\n",
        "foreground = labels[0].cpu().numpy().astype(np.uint8)\n",
        "plot_central_cuts(foreground)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxtPjdIDItfo"
      },
      "source": [
        "plot_predicted(inputs, foreground, 30, title=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqlxW7dBItfq"
      },
      "source": [
        "## 3. Patch-based segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQb_H1u4-Mzu"
      },
      "source": [
        "Let's fedine another experiment within the same workspace in `COMET ML`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-x4kQJm-NRG"
      },
      "source": [
        "experiment.set_name(\"Patch based, 64 batch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOzdJAykItfw"
      },
      "source": [
        "patch_size = 64\n",
        "samples_per_volume = 8\n",
        "max_queue_length = 240\n",
        "training_batch_size = 16\n",
        "validation_batch_size = 16\n",
        "\n",
        "patches_training_set = torchio.Queue(\n",
        "    subjects_dataset=training_set,\n",
        "    max_length=max_queue_length,\n",
        "    samples_per_volume=samples_per_volume,\n",
        "    sampler=torchio.sampler.UniformSampler(patch_size),\n",
        "    num_workers=multiprocessing.cpu_count(),\n",
        "    shuffle_subjects=True,\n",
        "    shuffle_patches=True,\n",
        ")\n",
        "\n",
        "patches_validation_set = torchio.Queue(\n",
        "    subjects_dataset=validation_set,\n",
        "    max_length=max_queue_length,\n",
        "    samples_per_volume=samples_per_volume,\n",
        "    sampler=torchio.sampler.UniformSampler(patch_size),\n",
        "    num_workers=multiprocessing.cpu_count(),\n",
        "    shuffle_subjects=False,\n",
        "    shuffle_patches=False,\n",
        ")\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    patches_training_set, batch_size=training_batch_size)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    patches_validation_set, batch_size=validation_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEOsawhiItf3"
      },
      "source": [
        "import SimpleITK as sitk\n",
        "\n",
        "def train(num_epochs, training_loader, validation_loader, model, optimizer, weights_stem):\n",
        "    run_epoch(0, Action.VALIDATE, validation_loader, model, optimizer)\n",
        "    for epoch_idx in range(1, num_epochs + 1):\n",
        "        print('Starting epoch', epoch_idx)\n",
        "        run_epoch(epoch_idx, Action.TRAIN, training_loader, model, optimizer)\n",
        "        run_epoch(epoch_idx, Action.VALIDATE, validation_loader, model, optimizer)\n",
        "        experiment.log_epoch_end(epoch_idx)\n",
        "        \n",
        "        \n",
        "def run_epoch(epoch_idx, action, loader, model, optimizer):\n",
        "    is_training = action == Action.TRAIN\n",
        "    epoch_losses = []\n",
        "    model.train(is_training)\n",
        "    for batch_idx, batch in enumerate(tqdm(loader)):\n",
        "        inputs, targets = prepare_batch(batch, device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(is_training):\n",
        "            logits = forward(model, inputs)\n",
        "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
        "            batch_losses = get_dice_loss(probabilities, targets)\n",
        "            batch_loss = batch_losses.mean()\n",
        "            if is_training:\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            epoch_losses.append(batch_loss.item())\n",
        "            if action == Action.TRAIN:\n",
        "                experiment.log_metric(\"train_dice_loss\", batch_loss.item())\n",
        "            elif action == Action.VALIDATE:\n",
        "                experiment.log_metric(\"validate_dice_loss\", batch_loss.item())\n",
        "    epoch_losses = np.array(epoch_losses)\n",
        "    \n",
        "    print(f'{action.value} mean loss: {epoch_losses.mean():0.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHEFs-YKItf5"
      },
      "source": [
        "model, optimizer, scheduler = get_model_and_optimizer(device)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "weights_stem = 'patches'\n",
        "train(num_epochs, training_loader, validation_loader, model, optimizer, weights_stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXH2vflNItf6"
      },
      "source": [
        "import nibabel as nib\n",
        "sample = random.choice(validation_set)\n",
        "input_tensor = sample[MRI][DATA][0]\n",
        "patch_size = 64, 64, 64  # we can user larger or smaller patches for inference\n",
        "patch_overlap = 4, 4, 4\n",
        "grid_sampler = torchio.inference.GridSampler(\n",
        "    sample,\n",
        "    patch_size,\n",
        "    patch_overlap,\n",
        ")\n",
        "patch_loader = torch.utils.data.DataLoader(\n",
        "    grid_sampler, batch_size=validation_batch_size)\n",
        "aggregator = torchio.inference.GridAggregator(grid_sampler)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for patches_batch in patch_loader:\n",
        "        inputs = patches_batch[MRI][DATA].to(device)\n",
        "        locations = patches_batch['location']\n",
        "        logits = model(inputs)\n",
        "        labels = logits.argmax(dim=CHANNELS_DIMENSION, keepdim=True)\n",
        "        aggregator.add_batch(labels, locations)\n",
        "plot_central_cuts(aggregator.get_output_tensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk2l3Yz2A3rP"
      },
      "source": [
        "Again we expect to see DICE > 0.85 after 40 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUSY0_Db9g3t"
      },
      "source": [
        "## 4. What else?\n",
        "1. Transforms (histogtam standartization)\n",
        "2. Augmentations for patch based segmentation\n",
        "3. Autoencoding architectures"
      ]
    }
  ]
}